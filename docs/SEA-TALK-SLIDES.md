# SEA Talk: Agentic Search - From Research to Production
**30-Minute Talk with Live Demo**

**Speaker**: Foyzul Karim
**Theme**: Bridging IR/NLP Research with Real-world Implementation

---

## Talk Structure (30 minutes)

```
Introduction (2 min)
  â†“
The Problem (3 min) + Quick Demo
  â†“
Architecture Overview (5 min)
  â†“
Live Demo: The 3-Node Pipeline (8 min) â† CORE OF TALK
  â†“
Research â†’ Implementation Lessons (8 min)
  â†“
Challenges & Open Questions (3 min)
  â†“
Q&A (remaining time)
```

---

# SLIDE DECK

## Slide 1: Title
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   AGENTIC SEARCH FOR AI TOOLS DISCOVERY                â”‚
â”‚   From Research Concepts to Production Reality         â”‚
â”‚                                                         â”‚
â”‚   Foyzul Karim                                          â”‚
â”‚   CodiesVibe.com                                        â”‚
â”‚                                                         â”‚
â”‚   Search Engines Amsterdam 2025                         â”‚
â”‚   Agentic Search and Reasoning Session                 â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Speaker Notes**:
- Introduce yourself briefly
- "Today I'll show you how we built an agentic search system for real users"
- "Focus on what worked, what didn't, and what we learned"

---

## Slide 2: The Challenge

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  THE PROBLEM: Traditional Search Fails                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Users Ask Natural Questions:                           â”‚
â”‚  âŒ "AI tools for code completion"                     â”‚
â”‚  âŒ "Local code assistants that are free"             â”‚
â”‚  âŒ "Tools like Cursor but cheaper"                    â”‚
â”‚                                                         â”‚
â”‚  Traditional Keyword Search:                            â”‚
â”‚  â†’ Misses semantic meaning                              â”‚
â”‚  â†’ Can't handle comparisons                             â”‚
â”‚  â†’ Ignores constraints (free, local, etc.)             â”‚
â”‚                                                         â”‚
â”‚  Scale:                                                 â”‚
â”‚  â€¢ 500+ AI tools                                        â”‚
â”‚  â€¢ 50+ attributes per tool                              â”‚
â”‚  â€¢ 1000s of queries/day                                 â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Speaker Notes**:
- "This is a real problem we faced at CodiesVibe"
- "Users don't speak database - they speak human"
- **[QUICK DEMO]**: Show traditional keyword search failing

**Demo Script (1 min)**:
1. Search "free AI code tools" on a traditional search
2. Show poor results (misses "free" constraint)
3. Transition: "We needed something better"

---

## Slide 3: Research Concepts We Used

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FROM RESEARCH TO PRACTICE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  ğŸ“š Research Concept        â†’    ğŸ”¨ Our Implementation   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                         â”‚
â”‚  Query Understanding        â†’    LLM Intent Extractor   â”‚
â”‚  (NLP, NLU)                                             â”‚
â”‚                                                         â”‚
â”‚  Dense Retrieval            â†’    Multi-Vector Search    â”‚
â”‚  (BERT, embeddings)              (Qdrant + 4 collections)â”‚
â”‚                                                         â”‚
â”‚  Result Fusion              â†’    Reciprocal Rank Fusion â”‚
â”‚  (CombMNZ, RRF)                  (RRF with k=60)        â”‚
â”‚                                                         â”‚
â”‚  Hybrid Search              â†’    Vector + Structured    â”‚
â”‚  (BM25 + Dense)                  (Qdrant + MongoDB)     â”‚
â”‚                                                         â”‚
â”‚  Agentic AI                 â†’    LangGraph 3-Node       â”‚
â”‚  (ReAct, Chain-of-Thought)       Pipeline               â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Speaker Notes**:
- "These are familiar concepts to you as researchers"
- "The challenge: How do you make them work together in production?"
- "Let me show you our architecture"

---

## Slide 4: System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CODIESVIBE ARCHITECTURE                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚                    User Query                           â”‚
â”‚                        â†“                                â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚              â”‚  LangGraph       â”‚                       â”‚
â”‚              â”‚  3-Node Pipeline â”‚                       â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                        â†“                                â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚        â†“               â†“               â†“               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚ Intent â”‚ â†’  â”‚  Query   â”‚ â†’  â”‚  Query   â”‚          â”‚
â”‚   â”‚Extract â”‚    â”‚ Planner  â”‚    â”‚ Executor â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚       â†“              â†“                â†“                â”‚
â”‚      LLM           LLM          â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”          â”‚
â”‚                                 â†“           â†“          â”‚
â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                            â”‚ Qdrant â”‚  â”‚MongoDB â”‚      â”‚
â”‚                            â”‚Vector  â”‚  â”‚Struct. â”‚      â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                 â†“           â†“          â”‚
â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                            â”‚  RRF Fusion     â”‚         â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                    â†“                   â”‚
â”‚                                Results                 â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Speaker Notes**:
- "Three-stage pipeline: Understand â†’ Plan â†’ Execute"
- "LLM nodes for reasoning, deterministic execution for reliability"
- "Let's see this in action with a live demo"

---

## Slide 5: LIVE DEMO Setup

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEMO: Let's Search Together                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Query: "Free AI code completion tools that work        â”‚
â”‚          locally"                                       â”‚
â”‚                                                         â”‚
â”‚  We'll watch the 3-node pipeline in action:             â”‚
â”‚                                                         â”‚
â”‚  1ï¸âƒ£  Intent Extraction                                 â”‚
â”‚     What did the LLM understand?                        â”‚
â”‚                                                         â”‚
â”‚  2ï¸âƒ£  Query Planning                                    â”‚
â”‚     How will we search?                                 â”‚
â”‚                                                         â”‚
â”‚  3ï¸âƒ£  Execution                                         â”‚
â”‚     What results do we get?                             â”‚
â”‚                                                         â”‚
â”‚  [Enable Debug Mode for visibility]                     â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Speaker Notes**:
- "I've enabled debug mode so we can see inside the pipeline"
- "This is a real query from a real user"
- **[Switch to live demo]**

---

## ğŸ¬ LIVE DEMO SCRIPT (8 minutes)

### Part 1: Intent Extraction (2 min)

**Action**: Enter query and pause after Intent Extractor

**Show on screen**:
```json
{
  "node": "intent-extractor",
  "output": {
    "primaryGoal": "find",
    "category": ["Code Completion", "AI"],
    "deployment": "Local",
    "pricing": "free",
    "desiredFeatures": ["Code Completion", "Local Inference"],
    "confidence": 0.92
  },
  "executionTime": "340ms"
}
```

**Talking points**:
- "The LLM extracted structured intent from natural language"
- "Notice: category, deployment, pricing - all mapped to our schema"
- "Confidence score helps us decide if we need fallback"

**Research â†’ Practice**:
- âœ… Research: Query understanding with LLMs
- ğŸ”¨ Practice: Schema-driven extraction with controlled vocabularies
- ğŸ’¡ Lesson: "Constrain the LLM's output space to reduce hallucinations"

---

### Part 2: Query Planning (3 min)

**Show on screen**:
```json
{
  "node": "query-planner",
  "output": {
    "strategy": "identity-focused",
    "vectorSources": [
      {
        "collection": "tools",
        "embeddingType": "semantic",
        "topK": 20
      },
      {
        "collection": "functionality",
        "embeddingType": "functional",
        "topK": 12
      }
    ],
    "structuredSources": [
      {
        "source": "mongodb",
        "filters": [
          { "field": "pricingSummary.hasFreeTier", "value": true },
          { "field": "deployment", "value": "Local" }
        ],
        "limit": 100
      }
    ],
    "fusion": "rrf",
    "confidence": 0.88,
    "explanation": "Using identity-focused strategy with semantic
                    search on tools collection + functional search
                    on functionality collection. MongoDB filters
                    enforce pricing and deployment constraints."
  },
  "executionTime": "420ms"
}
```

**Talking points**:
- "The planner chose an 'identity-focused' strategy"
- "It selected 2 vector collections + MongoDB structured search"
- "Notice: Different topK values - tools get higher priority"
- "Fusion method: RRF - I'll explain why we chose this"

**Research â†’ Practice**:
- âœ… Research: Federated search, collection selection
- ğŸ”¨ Practice: LLM decides which collections to query
- ğŸ’¡ Lesson: "LLMs are good at strategy, not execution"

**Why RRF? (30 seconds)**:
```
Research Options:
â€¢ Weighted Fusion â†’ Requires score normalization (fragile)
â€¢ CombMNZ â†’ Sensitive to outliers
â€¢ RRF â†’ Scale-invariant, simple, works well

Our Choice: RRF
â†’ No training data needed
â†’ Robust across different score types
â†’ Default parameters work well (k=60)
```

---

### Part 3: Execution (2 min)

**Show on screen**:
```json
{
  "node": "query-executor",
  "parallelQueries": [
    {
      "collection": "tools",
      "results": 20,
      "avgScore": 0.78,
      "time": "45ms"
    },
    {
      "collection": "functionality",
      "results": 12,
      "avgScore": 0.71,
      "time": "38ms"
    },
    {
      "source": "mongodb",
      "results": 8,
      "matchedFilters": ["hasFreeTier=true", "deployment=Local"],
      "time": "12ms"
    }
  ],
  "fusion": {
    "method": "rrf",
    "k": 60,
    "beforeFusion": 40,
    "afterFusion": 10,
    "duplicatesRemoved": 5
  },
  "totalTime": "95ms"
}
```

**Talking points**:
- "Parallel execution - all queries run simultaneously"
- "40 results â†’ RRF fusion â†’ 10 final results"
- "Deduplication removed 5 duplicates"
- "Total execution time: 95ms - much faster than LLM calls"

**Show final results** (scroll through top 3-5)

**Research â†’ Practice**:
- âœ… Research: Multi-source fusion
- ğŸ”¨ Practice: Parallel queries + RRF + deduplication
- ğŸ’¡ Lesson: "Deterministic execution is fast and debuggable"

---

### Part 4: Full Pipeline Timing (1 min)

**Show on screen**:
```
Pipeline Execution Breakdown:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Node                â”‚ Time     â”‚ % Total â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Intent Extractor    â”‚  340ms   â”‚  39%    â”‚
â”‚ Query Planner       â”‚  420ms   â”‚  48%    â”‚
â”‚ Query Executor      â”‚   95ms   â”‚  11%    â”‚
â”‚ Other (overhead)    â”‚   20ms   â”‚   2%    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL               â”‚  875ms   â”‚ 100%    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Bottleneck: LLM calls (87% of time)
```

**Talking points**:
- "LLM nodes take 87% of the time"
- "This is why we added caching - let me show you"

---

### Part 5: Cache Demo (1 min)

**Action**: Run the SAME query again

**Show on screen**:
```json
{
  "cacheHit": true,
  "cacheSimilarity": 1.0,
  "skippedNodes": ["intent-extractor", "query-planner"],
  "executionPath": ["cache-check", "query-executor", "cache-store"],
  "timing": {
    "cacheCheck": "15ms",
    "queryExecutor": "92ms",
    "total": "107ms"
  },
  "savings": {
    "timeReduction": "88%",
    "llmCallsAvoided": 2,
    "estimatedCostSaved": "$0.0004"
  }
}
```

**Talking points**:
- "Cache hit! Query executed in 107ms vs 875ms"
- "88% faster, avoided 2 LLM calls"
- "In production, 70% cache hit rate â†’ massive cost savings"

**Research â†’ Practice**:
- âœ… Research: Query similarity, semantic caching
- ğŸ”¨ Practice: Vector-based cache lookup in MongoDB
- ğŸ’¡ Lesson: "Caching is essential for LLM-based systems in production"

**[End of Demo - Return to Slides]**

---

## Slide 6: Research â†’ Implementation Lessons

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WHAT WE LEARNED: 5 KEY LESSONS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  1ï¸âƒ£  SCHEMA-DRIVEN DESIGN                              â”‚
â”‚     Research: Controlled vocabularies, ontologies       â”‚
â”‚     Practice: DomainSchema as configuration             â”‚
â”‚     Lesson: "Make domain knowledge explicit and         â”‚
â”‚              portable"                                  â”‚
â”‚                                                         â”‚
â”‚  2ï¸âƒ£  HYBRID IS BETTER THAN PURE                        â”‚
â”‚     Research: Dense retrieval vs sparse retrieval       â”‚
â”‚     Practice: Vector search + structured filters        â”‚
â”‚     Lesson: "Don't choose - combine strengths"          â”‚
â”‚                                                         â”‚
â”‚  3ï¸âƒ£  LLMS FOR REASONING, NOT RETRIEVAL                 â”‚
â”‚     Research: LLMs can do everything                    â”‚
â”‚     Practice: LLMs understand, databases retrieve       â”‚
â”‚     Lesson: "Use the right tool for the job"            â”‚
â”‚                                                         â”‚
â”‚  4ï¸âƒ£  CACHING IS NOT OPTIONAL                           â”‚
â”‚     Research: Query optimization, materialization       â”‚
â”‚     Practice: Semantic cache with vector similarity     â”‚
â”‚     Lesson: "Production systems need speed AND quality" â”‚
â”‚                                                         â”‚
â”‚  5ï¸âƒ£  DATA SYNC IS HARDER THAN IT LOOKS                 â”‚
â”‚     Research: "Just index your vectors"                 â”‚
â”‚     Practice: MongoDBâ†’Qdrant smart sync + retry logic   â”‚
â”‚     Lesson: "Two databases = synchronization problem"   â”‚
â”‚                                                         â”‚
â”‚  6ï¸âƒ£  INTERPRETABILITY > BLACK BOX                      â”‚
â”‚     Research: Explainable AI, model transparency        â”‚
â”‚     Practice: 3-node pipeline with visible outputs      â”‚
â”‚     Lesson: "Debugging requires observability"          â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Speaker Notes** (spend 1-2 min on each lesson):

### Lesson 1: Schema-Driven Design (1.5 min)

"Let me show you what I mean by schema-driven..."

**Show code snippet**:
```typescript
// Before: Hardcoded in prompts (bad)
const prompt = `
  Categories: AI, Code Editor, IDE, Chatbot, ...
  Features: Code Generation, Debugging, ...
`;

// After: Schema configuration (good)
const toolsSchema = {
  vocabularies: {
    categories: ['AI', 'Code Editor', 'IDE', ...],
    functionality: ['Code Generation', 'Debugging', ...]
  }
};

// Prompts generated dynamically
const prompt = generatePrompt(toolsSchema);
```

**Why this matters**:
- âœ… Single source of truth
- âœ… Easy to add new domains (recipes, products, etc.)
- âœ… Type-safe across the entire pipeline
- âŒ Initial refactoring effort

---

### Lesson 2: Hybrid Search (1.5 min)

**Show comparison**:
```
Query: "Free tools under $10/month"

Pure Vector Search:
âŒ "Free" and "$10/month" are contradictory
âŒ Embedding can't capture exact constraints
âŒ Results include expensive tools

Hybrid (Vector + Structured):
âœ… Vector: Semantic understanding of "tools"
âœ… MongoDB: Exact filter on price
âœ… Results: Only tools matching both
```

**Why this matters**:
- Research loves "pure" approaches
- Production needs "whatever works"
- Hybrid = best of both worlds

---

### Lesson 3: LLMs for Reasoning (1.5 min)

**Show the separation**:
```
Bad Approach:
Query â†’ LLM â†’ "Here are 10 tools..." â†’ Parse response
âŒ LLM generates tool names (hallucinations)
âŒ No guarantee tools exist
âŒ Expensive, slow

Our Approach:
Query â†’ LLM â†’ {intent JSON} â†’ Database â†’ Real results
âœ… LLM only does understanding
âœ… Database guarantees real data
âœ… Cheaper, faster, reliable
```

**Why this matters**:
- LLMs are great at reasoning
- LLMs are bad at facts (without RAG)
- Separate concerns = better system

---

### Lesson 4: Caching (1.5 min)

**Show the math**:
```
Production Reality:
â€¢ 1000 queries/day
â€¢ Many similar queries: "free AI tools", "free ai tools",
  "free AI tools for developers"
â€¢ Without cache: 1000 Ã— 2 LLM calls Ã— $0.0001 = $0.20/day
â€¢ With cache (70% hit): 300 Ã— 2 Ã— $0.0001 = $0.06/day
â€¢ Savings: $0.14/day = $51/year

Latency:
â€¢ Without cache: 800-1200ms
â€¢ With cache: 100-200ms
â€¢ User experience: Night and day difference
```

**Why this matters**:
- Research papers rarely mention caching
- Production systems live or die on latency
- Semantic caching is powerful but underutilized

---

### Lesson 5: Data Synchronization (1.5 min)

"Research papers say 'build a vector index' - but production means keeping TWO databases in sync..."

**The Problem**:
```
User creates tool in Admin UI
  â†“
MongoDB updated âœ…
  â†“
Qdrant vector index... ğŸ¤”
  â†“
Is it updated? Which collections? What if it fails?
```

**Our Solution: Smart Sync System**:
```typescript
// When tool is created/updated
1. Save to MongoDB (ALWAYS succeeds)
2. Calculate content hash per collection
3. Async sync to Qdrant (4 collections)
4. Track status per collection
5. Background worker retries failures

// Each collection tracks its own sync state
syncMetadata: {
  collections: {
    tools: { status: "synced", hash: "abc123" },
    functionality: { status: "pending", retryCount: 2 },
    usecases: { status: "failed", error: "timeout" },
    interface: { status: "synced", hash: "def456" }
  }
}
```

**Key Features**:
- **Change detection**: Only sync collections with changed fields
- **Failure isolation**: MongoDB succeeds even if Qdrant fails
- **Background worker**: Auto-retry with exponential backoff (1 min â†’ 1 hour)
- **Observability**: Admin dashboard shows sync health per tool

**Why this matters**:
- Research: "Just index your vectors" âœ¨
- Production: Sync failures, retries, monitoring, debugging ğŸ˜…
- Two databases = twice the complexity
- This is why we need sync metrics, status tracking, and admin tools

---

### Lesson 6: Interpretability (1.5 min)

**Show debugging example**:
```
User: "Results are wrong!"

Black Box System:
â“ Which component failed?
â“ Was it embedding? Ranking? Filtering?
â“ How do we fix it?

Our 3-Node System:
âœ… Check intent extraction â†’ Correct? Yes.
âœ… Check query plan â†’ Selected right collections? No!
âœ… Found the bug â†’ Planner logic issue
âœ… Fix and redeploy
```

**Why this matters**:
- End-to-end models are elegant
- Production needs debuggability
- Visible intermediate steps = faster debugging

---

## Slide 7: Practical Challenges

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CHALLENGES WE FACED                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  âš ï¸  Challenge              â†’    âœ… Our Solution        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                         â”‚
â”‚  LLM Costs Adding Up        â†’    Semantic cache        â”‚
â”‚                                  (70% hit rate)         â”‚
â”‚                                                         â”‚
â”‚  LLM Hallucinations         â†’    Schema validation +   â”‚
â”‚                                  controlled vocabs      â”‚
â”‚                                                         â”‚
â”‚  Multi-Collection           â†’    RRF fusion            â”‚
â”‚  Different Score Scales          (scale-invariant)      â”‚
â”‚                                                         â”‚
â”‚  Empty Results              â†’    Progressive filter    â”‚
â”‚  (Over-constrained)              relaxation             â”‚
â”‚                                                         â”‚
â”‚  Slow Cold Starts           â†’    Warm cache on deploy  â”‚
â”‚                                  + async embedding      â”‚
â”‚                                                         â”‚
â”‚  MongoDB-Qdrant Sync        â†’    Smart sync service +  â”‚
â”‚  (Two databases drift)           background worker +    â”‚
â”‚                                  per-collection status  â”‚
â”‚                                                         â”‚
â”‚  How to Evaluate?           â†’    User clicks +         â”‚
â”‚  (No Ground Truth)               diversity metrics +    â”‚
â”‚                                  test query suite       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Speaker Notes** (2 min):
- "These are problems you won't find in research papers"
- "But they're critical for production systems"
- "Happy to discuss any of these in Q&A"

---

## Slide 8: Open Research Questions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WHAT WE STILL DON'T KNOW                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  1. How do we evaluate agentic search quality?          â”‚
â”‚     â€¢ No standard benchmarks                            â”‚
â”‚     â€¢ User intent is subjective                         â”‚
â”‚     â€¢ What metrics matter?                              â”‚
â”‚                                                         â”‚
â”‚  2. Can we learn query planning from user behavior?     â”‚
â”‚     â€¢ RLHF for query planner?                           â”‚
â”‚     â€¢ Implicit feedback signals?                        â”‚
â”‚     â€¢ Cold start problem?                               â”‚
â”‚                                                         â”‚
â”‚  3. How many agents is optimal?                         â”‚
â”‚     â€¢ We use 3 nodes - is that right?                   â”‚
â”‚     â€¢ More agents = more interpretable?                 â”‚
â”‚     â€¢ Latency vs modularity trade-off?                  â”‚
â”‚                                                         â”‚
â”‚  4. Cross-domain transfer?                              â”‚
â”‚     â€¢ Schema works for tools - what about products?     â”‚
â”‚     â€¢ Recipes? Documents? Medical data?                 â”‚
â”‚     â€¢ Universal schema possible?                        â”‚
â”‚                                                         â”‚
â”‚  5. How to handle evolving schemas?                     â”‚
â”‚     â€¢ New categories emerge                             â”‚
â”‚     â€¢ User language changes                             â”‚
â”‚     â€¢ Cache invalidation strategies?                    â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Speaker Notes** (2 min):
- "These are genuine open questions"
- "Would love to collaborate with researchers here"
- "If any of these interest you, let's talk after"

---

## Slide 9: Key Takeaways

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WHAT TO REMEMBER                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  FOR RESEARCHERS:                                       â”‚
â”‚  â€¢ Agentic search is interpretable and modular          â”‚
â”‚  â€¢ Hybrid approaches often beat pure methods            â”‚
â”‚  â€¢ We need better evaluation frameworks                 â”‚
â”‚                                                         â”‚
â”‚  FOR PRACTITIONERS:                                     â”‚
â”‚  â€¢ Schema-driven design enables portability             â”‚
â”‚  â€¢ LLMs for reasoning, databases for retrieval          â”‚
â”‚  â€¢ Caching is essential for production                  â”‚
â”‚                                                         â”‚
â”‚  FOR EVERYONE:                                          â”‚
â”‚  â€¢ Real systems teach us what matters                   â”‚
â”‚  â€¢ Research â†” Practice gap is real but bridgeable      â”‚
â”‚  â€¢ Production constraints drive innovation              â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Research without implementation is incomplete  â”‚   â”‚
â”‚  â”‚  Implementation without research is fragile     â”‚   â”‚
â”‚  â”‚                                                 â”‚   â”‚
â”‚  â”‚  â†’ We need both                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Speaker Notes** (1 min):
- "This is what I hope you take away"
- "Research and practice need each other"
- "CodiesVibe is open source - please try it, break it, improve it"

---

## Slide 10: Try It Yourself

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RESOURCES                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  ğŸŒ Live Demo                                           â”‚
â”‚     codiesvibe.com                                      â”‚
â”‚     (Try the queries we used today)                     â”‚
â”‚                                                         â”‚
â”‚  ğŸ’» Source Code                                         â”‚
â”‚     github.com/foyzulkarim/codiesvibe                   â”‚
â”‚     â†’ search-api/ directory                             â”‚
â”‚     â†’ Full LangGraph implementation                     â”‚
â”‚                                                         â”‚
â”‚  ğŸ“š Documentation                                       â”‚
â”‚     /docs/SEA-TALK-PREPARATION.md                       â”‚
â”‚     â†’ Architecture deep-dive                            â”‚
â”‚     â†’ Implementation details                            â”‚
â”‚     â†’ Q&A preparation                                   â”‚
â”‚                                                         â”‚
â”‚  ğŸ“§ Contact                                             â”‚
â”‚     [Your email]                                        â”‚
â”‚     [Your Twitter/LinkedIn]                             â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           [QR Code Here]                        â”‚   â”‚
â”‚  â”‚       â†’ GitHub Repository                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                         â”‚
â”‚  Questions? Let's discuss!                              â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Speaker Notes**:
- "All code is open source"
- "Documentation includes everything I couldn't cover today"
- "I'm here for Q&A - please ask anything!"

---

# DEMO PREPARATION CHECKLIST

## Before the Talk

### System Preparation
- [ ] Deploy to stable server (not localhost)
- [ ] Warm the cache with these queries:
  ```
  - "free AI code completion tools"
  - "AI tools for code generation that work locally"
  - "tools like Cursor but cheaper"
  - "collaborative development tools"
  ```
- [ ] Test debug mode endpoint
- [ ] Verify vLLM is responsive
- [ ] Check MongoDB and Qdrant connections

### Backup Preparation
- [ ] Record demo video as backup
- [ ] Take screenshots of each demo stage
- [ ] Prepare fallback slides with static results
- [ ] Test on conference WiFi (if possible)

### Demo Environment
- [ ] Browser: Chrome/Firefox with dev tools ready
- [ ] Terminal: For showing logs if needed
- [ ] Screen resolution: Readable from back of room
- [ ] Font size: Increase for visibility
- [ ] Zoom: Prepare to zoom in on JSON

### Data Preparation
- [ ] Verify test queries return good results
- [ ] Check that cache hit/miss works correctly
- [ ] Ensure timing data is reasonable
- [ ] Test with edge cases (empty results, errors)

## Demo Flow

```
1. Show CodiesVibe homepage (10 sec)
   â†“
2. Enter query in search box (5 sec)
   â†“
3. Switch to debug view / DevTools (5 sec)
   â†“
4. Explain Intent Extraction output (2 min)
   â†“
5. Explain Query Planning output (3 min)
   â†“
6. Explain Execution output (2 min)
   â†“
7. Show final results (30 sec)
   â†“
8. Re-run same query for cache demo (1 min)
   â†“
9. Compare timing (30 sec)
```

**Total**: ~8 minutes

## If Demo Fails

**Plan B**: Use screenshots
- Have full demo flow captured
- Walk through as if live
- Acknowledge: "This is from our test run earlier"

**Plan C**: Skip to results
- Show just final output
- Focus on architecture explanation

## Demo Tips

### Before Each Demo Step
1. **Narrate what you're doing**: "Now I'm going to enter this query..."
2. **Pause for effect**: Give audience time to read output
3. **Highlight key parts**: Point or circle important fields
4. **Connect to research**: "This is where RRF happens..."

### During Demo
- **Speak slowly**: Audience is reading and listening
- **Read important values aloud**: "Notice the confidence is 0.92"
- **Explain abbreviations**: "RRF means Reciprocal Rank Fusion"
- **Show, don't just tell**: Click through, don't just describe

### After Demo
- **Summarize what we saw**: "So in 8 minutes we saw the full pipeline"
- **Bridge to next section**: "Now let's talk about what we learned..."

---

# TIMING BREAKDOWN (30 minutes)

```
00:00 - 02:00  Introduction + Problem Statement
02:00 - 03:00  Quick Demo: Keyword Search Failure
03:00 - 05:00  Architecture Overview
05:00 - 13:00  LIVE DEMO (Core of Talk)
               â”œ 05:00-07:00  Intent Extraction
               â”œ 07:00-10:00  Query Planning
               â”œ 10:00-12:00  Execution
               â”” 12:00-13:00  Cache Demo
13:00 - 21:30  Research â†’ Implementation Lessons (6 lessons)
21:30 - 24:00  Challenges & Open Questions
24:00 - 26:00  Key Takeaways
26:00 - 27:00  Resources
27:00 - 30:00  Buffer / Early Q&A
```

**Adjust timing during talk**:
- If running fast: Expand demo explanations
- If running slow: Compress lessons section
- Always leave 3-5 min for questions

---

# AUDIENCE ENGAGEMENT

## During Demo
- **Ask rhetorical questions**: "What do you think the LLM will extract here?"
- **Pause for reading**: "Take a moment to read this JSON output"
- **Invite comments**: "Anyone notice anything interesting?"

## During Lessons
- **Poll the room**: "How many of you have used RRF?"
- **Share anecdotes**: "We actually tried X first and it failed because..."
- **Connect to their work**: "If you're working on Y, this might help with..."

## Open Questions Slide
- **Genuinely ask**: "I'd love your thoughts on this"
- **Invite collaboration**: "If this interests you, let's talk"
- **Be humble**: "We don't have all the answers"

---

# ANTICIPATED QUESTIONS & ANSWERS

## Q: Why not use GPT-4 or Claude for everything?

**A**: "Great question. We actually tried that initially. The problems:
1. Cost - would be 10-20x more expensive
2. Context limits - can't fit all 500 tools
3. Hallucinations - LLM might invent tools
4. Latency - much slower than database queries

Our hybrid approach uses LLMs for what they're good at - understanding and planning - and databases for what they're good at - retrieval and filtering. Best of both worlds."

## Q: How do you handle query variations like typos?

**A**: "Two strategies:
1. Embedding models are naturally robust to small variations
2. Cache similarity matching catches near-duplicates

For example, 'AI tools' and 'AI tols' have similar embeddings. The cache will likely hit. For more severe typos, we could add fuzzy matching, but haven't needed it yet in practice."

## Q: What about multilingual queries?

**A**: "Currently English-only because our tool descriptions are in English. But the architecture supports multilingual with:
1. Multilingual embedding models
2. Language detection in intent extraction
3. Translation at the boundary

The schema-driven design actually makes this easier - we could define language-specific vocabularies."

## Q: How do you keep MongoDB and Qdrant in sync?

**A**: "Great question - this is one of those production problems research papers don't talk about! We built a Smart Sync system with several key features:

1. **Async fire-and-forget**: When a tool is created/updated, MongoDB saves immediately, then we trigger async sync to Qdrant - so users don't wait
2. **Per-collection tracking**: Each of our 4 Qdrant collections tracks its own sync status (synced, pending, failed)
3. **Change detection**: We calculate content hashes per collection and only sync what changed. For example, updating pricing doesn't re-embed the functionality collection
4. **Background worker**: Runs every 60 seconds, retries failed syncs with exponential backoff (1 min â†’ 1 hour), max 5 retries
5. **Failure isolation**: MongoDB writes always succeed even if Qdrant fails - we don't want to lose data

The admin dashboard shows sync health per tool, and we have API endpoints to manually trigger retries. This adds complexity, but it's essential for production reliability."

## Q: How do you validate that the LLM extracted intent correctly?

**A**: "Three validation layers:
1. Schema validation - reject invalid JSON structure
2. Vocabulary validation - check against controlled lists
3. Confidence thresholding - low confidence triggers fallback

We also log all extractions and manually review a sample weekly to catch systematic issues."

## Q: Why 3 nodes? Why not 2 or 5?

**A**: "Honest answer: Engineering intuition, not rigorous science. We separated intent and planning because:
1. Different concerns (understanding vs strategy)
2. Different cache granularities
3. Different optimization opportunities

Could you do it with 2? Probably. With 5? Maybe too complex. This felt like the right balance, but I'd love to see research on optimal agent decomposition."

## Q: What's your biggest regret or what would you change?

**A**: "I wish we'd built the schema-driven architecture from day one. We spent months with hardcoded prompts before refactoring. The schema approach is so much better but took significant effort to migrate.

Lesson: Think about extensibility early, not late."

---

# POST-TALK TODO

## Immediate (At Conference)
- [ ] Collect business cards / contacts
- [ ] Note interesting questions for follow-up
- [ ] Connect with potential collaborators
- [ ] Get feedback on presentation

## Within 1 Week
- [ ] Send follow-up emails to interested researchers
- [ ] Post slides + video on GitHub
- [ ] Write blog post summarizing talk
- [ ] Address any bugs found during demo

## Within 1 Month
- [ ] Implement suggested improvements
- [ ] Explore research collaborations
- [ ] Consider paper submission to workshop/conference
- [ ] Create benchmark dataset if there's interest

---

**You've got this! The key is showing genuine enthusiasm for bridging research and practice. Good luck! ğŸš€**
